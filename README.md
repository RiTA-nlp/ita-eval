# ItaEval

[![Leaderboard](https://img.shields.io/badge/Leaderboard-yellow)](https://huggingface.co/spaces/RiTA-nlp/ita-eval)
[![Technical Report](https://img.shields.io/badge/Report-green)](https://rita-nlp.org/)

This repository contains the configuration and code utilities to run the ItaEval evaluation suite.

- The repository is a fork of the [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness). Our last alignment happended on [1980a13](https://github.com/EleutherAI/lm-evaluation-harness/tree/1980a13c9d7bcdc6e2a19228c203f9f7834ac9b8).
- The suite is backing a live leaderboard [HERE](https://huggingface.co/spaces/RiTA-nlp/ita-eval) 

<!-- ## Cite as

```
@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 12,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836},
  url          = {https://zenodo.org/records/10256836}
} -->
```
