task: hatecheck_ita
group: toxicity_ita

dataset_path: Paul/hatecheck-italian
dataset_name: null
dataset_kwargs: null

training_split: null
validation_split: null
test_split: test

fewshot_split: test
num_fewshot: 5

output_type: multiple_choice

doc_to_text: "La seguente frase contiene contenuto d'odio? Rispondi Sì o No.\nFrase: {{test_case}}\nRisposta:"
doc_to_target: hateful
doc_to_choice: ['No', 'Sì']

process_docs: !function utils.process_docs
# process_results: !function utils.process_results

metric_list:
  - metric: f1
    aggregation: !function ../metrics.macro_f1_score
    higher_is_better: true

metadata:
  version: 0
